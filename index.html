<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:16px;
		margin-left: auto;
		margin-right: auto;
		width: 800px;
	}
	
	h1 {
		font-weight:300;
	}
		
	h2 {
		font-weight:300;
		font-size: 22px;
		text-align: left;
	}

	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}

	pre {
    text-align: left;
    white-space: pre;
	background-color: ghostwhite;
	border: 1px solid #CCCCCC;
	padding: 10px 20px;
	margin: 10px;
    tab-size:         4; /* Chrome 21+, Safari 6.1+, Opera 15+ */
    -moz-tab-size:    4; /* Firefox 4+ */
    -o-tab-size:      4; /* Opera 11.5 & 12.1 only */
  	}

</style>

<html>
  <head>
		<title>MAPLE: Masked Pseudo-Labeling autoEncoder for Semi-supervised Point Cloud Action Recognition</title>
		<meta property="og:image" content=""/>
		<meta property="og:title" content="MAPLE: Masked Pseudo-Labeling autoEncoder for Semi-supervised Point Cloud Action Recognition" />
  </head>

  <body>
    <br>
          <center>
          	<span style="font-size:42px">MAPLE: Masked Pseudo-Labeling autoEncoder for Semi-supervised Point Cloud Action Recognition</span>
	  		  <table align=center width=900px>
	  			  <tr>
	  	              <td align=center width=170px>
	  					<center>
	  						<span style="font-size:20px"><a href="https://github.com/">Xiaodong Chen<sup>1,2</sup></a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=140px>
	  					<center>
	  						<span style="font-size:20px"><a href="https://github.com/">Xinchen Liu<sup>2</sup></a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=100px>
	  					<center>
	  						<span style="font-size:20px"><a href="https://github.com/">Wu Liu<sup>2</sup></a></span>
		  		  		</center>
		  		  	  </td>
					<td align=center width=170px>
						<center>
							<span style="font-size:20px"><a href="https://github.com/">Yongdong Zhang<sup>1</sup></a></span>
						</center>
					</td>
					<td align=center width=170px>
						<center>
							<span style="font-size:20px"><a href="https://github.com/">Jungong Han<sup>3</sup></a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:20px"><a href="https://github.com/">Tao Mei<sup>2</sup></a></span>
						</center>
					</td>
				</table>
          		<!-- <span style="font-size:30px">ACM MM 2022.</span> -->

			  <table align=center width=900px>
				  <tr>
					  <td align=center width=400px>
						<center>
							<span style="font-size:18px"><a href="https://github.com/"><sup>1</sup>University Of Science And Technology Of China</a></span>
						</center>
					  </td>
					  <td align=center width=250px>
						<center>
							<span style="font-size:18px"><a href="https://github.com/"><sup>2</sup>AI Research of JD.com</a></span>
						</center>
					  </td>
					  <td align=center width=250px>
						<center>
							<span style="font-size:18px"><a href="https://github.com/"><sup>3</sup>Aberystwyth University</a></span>
						</center>
					  </td>
			  </table>
			  ACM Multimedia 2022 (<a href="https://2022.acmmm.org/" target="_blank">ACM MM</a>) 2022, <font color="#e86e14">Poster Presentation</font>
          </center>

   		  <br><br>
		  <hr>

  		  <br>
  		  <!-- <table align=center width=720px>
  			  <tr>
  	              <td width=400px>
  					<center>
  	                	<a href="./resources/images/Figure1.png" ><img class="rounded" src = "./resources/images/Figure1.png" width="95%" ></img></href></a><br>
					</center>
  	              </td>
                </tr>
  	              <td width=400px>
  					<center>
  	                	<span style="font-size:14px"><i>Figure 1</i>
					</center>
  	              </td>

  		  </table> -->
      	  <br><br>

		  <table align=center width=720px>
			<!-- <center><h1>Download</h1></center> -->
			<tr>
				<td width=300px>
					<center>
						<a href="#download"><img class="rounded" onmouseover="this.src='./resources/images/data_icon.png';" onmouseout="this.src='./resources/images/data_icon.png';" src = "./resources/images/data_icon.png" height = "120px"></a><br>
						<span style="font-size:16px">Download</span><br>
					</center>
				</td>
				<td width=300px>
					<center>
						<a href="#video"><img class="rounded" onmouseover="this.src='./resources/images/video_icon.png';" onmouseout="this.src='./resources/images/video_icon.png';" src = "./resources/images/video_icon.png" height = "120px"></a><br>
						<span style="font-size:16px">Videos at ACM MM'2022</span><br>
					</center>
				</td>
				<td width=300px>
					<center>
						<a href="#analysis"><img class="rounded" onmouseover="this.src='./resources/images/magnify_glass.png';" onmouseout="this.src='./resources/images/magnify_glass.png';" src = "./resources/images/magnify_glass.png" height = "120px"></a><br>
						<span style="font-size:16px">Analysis</span><br>
					</center>
				</td>
				<td width=300px>
					<center>
					  <a href="https://github.com/SheldongChen/MAPLE"><img class="rounded" onmouseover="this.src='./resources/images/github_icon.png';" onmouseout="this.src='./resources/images/github_icon.png';" src = "./resources/images/github_icon.png" height = "120px"></a><br>
					  <span style="font-size:16px">GitHub Repo</span><br>						
					</center>
				</td>
			</tr>
		  </table>

		  <br><br>

		  <hr>

  		  <table align=center width=720px>
				<center><h1>Abstract</h1></center>
		  </table>
		  <center>
			<span>
				Recognizing human actions from point cloud videos has attracted tremendous attention from both academia and industry due to its wide applications like automatic driving, robotics, and human-computer interaction.
				However, current methods for point cloud action recognition usually require a huge amount of data with manual annotations and a complex backbone network with high computation cost, which makes it impractical for real-world applications.
				Therefore, this paper considers the task of semi-supervised point cloud action recognition with an efficient model.
				To this end, we propose a Masked Pseudo-Labeling autoEncoder (<b>MAPLE</b>) framework to learn effective representations with much fewer annotations for point cloud action recognition.
				In particular, we design a novel and efficient <b>De</b>coupled <b>s</b>patial-<b>t</b>emporal Trans<b>Former</b> (<b>DestFormer</b>) as the backbone of MAPLE.
				In DestFormer, the spatial and temporal dimensions of the 4D point cloud videos are decoupled to achieve an efficient self-attention for learning both long-term and short-term features.
				Moreover, to learn discriminative features from fewer annotations, we design a masked pseudo-labeling autoencoder structure to guide the DestFormer to reconstruct features of masked frames from the available frames.
				More importantly, for unlabeled data, we exploit the pseudo-labels from the classification head as the supervision signal for the reconstruction of features from the masked frames.
				Finally, comprehensive experiments demonstrate that MAPLE achieves superior results on three public benchmarks and outperforms the state-of-the-art method by 8.08% accuracy on the MSR-Action3D dataset.
		  	</span>
		  </center>
  		  <br><br>
		  <hr>

		

		  <table align=center width=720px>
			<center><h1>5-Minute presentation video (TBD)</h1></center>
			<tr>
				<table align=center width=720px>
					<tr>
						<td align=center width=720px>
							<iframe width="600" height="320" src="https://www.youtube.com/watch?v=" frameborder="0" allowfullscreen></iframe> 
						</td>
					  </tr>
					<tr>
					 </table>
			  </tr>
		  </table>
		   <br><br>
		  <hr>


		  <table align=center width=720px>
			<center><h1>Architecture of DestFormer</h1></center>
			<tr>
				<td width=400px>
				  <center>
					  <a><img class="rounded" src = "./resources/images/Figure2.png" width="800px"></img></a><br>
				</center>
				</td>
			</tr>
				<td width=400px>
				  <center>
					  <span style="font-size:14px">
						<div class="formula">
						The detail of DestFormer. 
						(a) <i>Data Preparation</i>: we construct some local areas (e.g. ``a'') on adjacent frames (e.g. ``t1'', ``t2'') from the input `x_i` as what P4Conv do. 
						(b) <i>Spatial Extractor</i>: we adopt P4Conv for modeling short-time local information and feed the output `s_i` frame by frame into a spatial transformer for extracting the merged local feature `m_i`. 
						(c) <i>Temporal Aggregator</i>: we generate the short-term global feature `g_i` through the pooling layer and aggregate the long-term global information with the temporal encoder. 
						(d) <i>Prediction Head</i>: we project the global feature `v_i` into label space via the classification head.
						</div>
				</center>
				</td>

		  </table>
	      <br><br>
		  <hr>

		  <table align=center width=720px>
			<center><h1>Architecture of MAPLE</h1></center>
			<tr>
				<td width=400px>
				  <center>
					  <a><img class="rounded" src = "./resources/images/Figure3.png" width="800px"></img></a><br>
				</center>
				</td>
			</tr>
				<td width=400px>
				  <center>
					  <span style="font-size:14px"><div class="formula">
						The detail of our MAPLE. 
						(<i>a</i>) Adopting our DestFormer backbone and the cross-entropy loss for the supervised training. 
						(<i>b</i>) The complete training process of MAPLE: 
						(1) The spatial extractor encodes the input video as the short-term global feature `g_i`. 
						(2) After randomly discarding the short-term global feature `g_i`, the temporal encoder projects the visible subset of `g_i` as the latent representation `z_i`. 
						(3) The temporal decoder is responsible for reconstructing `r_i` from the latent representation `z_i` and the mask tokens `M`. 
						(4) The classification head generates the pseudo-label `P_i` and `\hat{P}_i` as our reconstruction target. Note that the modules here with the same colors share weights.
					</div>
				</center>
				</td>

		  </table>
		  <br><br>
		  <hr>



		  <table id='analysis' align=center width=720px>
			<center><h1>Evaluation and Visualization</h1></center>




			<table align=center width=720px>
				<center><h1>The results of MAPLE</h1></center>
				<tr>
					<center>
					<span style="font-size:18px">
						The Semi-supervised results of MAPLE on MSR-Action3D and NTU RGB+D 60</span>
					<br>
					</center>
					<td width=720px>
						<center>
							<a><img class="rounded" src = "./resources/images/Figure4.png" width="360px"></img></a><br>
							<span style="font-size:18px"><a>MSR-Action3D</a></span><br>
						</center>
					</td>
					<td width=360px>
						<center>
							<a><img class="rounded" src = "./resources/images/Figure5.png" width="360px"></img></a><br>
							<span style="font-size:18px"><a>NTU RGB+D 60</a></span><br>
						</center>
					</td>
				</tr>
			  </table>
	      <br><br>
		  <hr>
		  
		  <table align=center width=720px>
			<center><h1>Ablation Study</h1></center>
			<tr>
				<center>
				<span style="font-size:22px">
					The influence of the masking ratio and the depth of the temporal decoder.</span>
				<br>
				</center>
				<td width=720px>
					<center>
						<a><img class="rounded" src = "./resources/images/Figure6.jpg" width="360px"></img></a><br>
						<span style="font-size:18px"><a>The accuracy of classification on NTU RGB+D 60 5% labeled dataset with different masking ratios.</a></span><br>
					</center>
				</td>
				<td width=360px>
					<center>
						<a><img class="rounded" src = "./resources/images/Figure7.jpg" width="360px"></img></a><br>
						<span style="font-size:18px"><a> The accuracy of classification on NTU RGB+D 60 5% labeled dataset with different depth of temporal decoder.</a></span><br>
					</center>
				</td>
			</tr>
		  </table>

		  <br><br>
		  <hr>

		  <table id="download" align=center width=720px>
			<center><h1>Download</h1></center>
			<tr>
		
				<td width=300px>
					<center>
						<span style="font-size:24px">The division of each semi-supervised datasets</span><br><br>
						<img class="rounded" onmouseover="this.src='./resources/images/dataset_icon.jpg';" onmouseout="this.src='./resources/images/dataset_icon.jpg';" src = "./resources/images/dataset_icon.jpg" height = "150px"><br><br>
						<span style="font-size:16px"><a href='resources/dataset/msraction3d/msr.zip'>MSR-Action3D</a></span><br>
						<span style="font-size:16px"><a href='resources/dataset/ntu60/ntu60.zip'>NTU RGB+D 60</a></span><br>
						<span style="font-size:16px"><a href='resources/dataset/ntu120/ntu120.zip'>NTU RGB+D 120</a></span><br>
					<span style="font-size:16px"></span>
					</center>
				</td>
				<table>
				<center><h2> Updates </h2></center>
				[30/06/2022] We include new subsections to track updates and address FAQs.<br>
				<center><h2> FAQs </h2></center>
				<br>
				Q1: Explain the trend in Figure 6.<br><br>
				A1: As discussed in Section 4.5, the 75% masking ratio of reconstruction achieves the peak of classification performance. 
				This trend of the plot in Figure 6 can be explained in terms of information density [1]. 
				Similar to the Masked AutoEncoder (MAE) for image classification and video recognition [1][2][3], point cloud videos tend to have severe spatial-temporal redundancy. 
				With this characteristic, the masked short-term features can be reconstructed from the small visible features by a high-level understanding of human actions.<br><br>
				
				Q2: Why do the figures shown in the appendix use the L2 norm instead of the optimization function used in the paper? 
				Does the exploding and vanishing problem only occur when using the L2 norm?<br><br>
				A2: <br>
				(1). The purpose of the figure shown in the appendix is to explore the stability of network training under different training strategies. 
				The exploding and vanishing problem is not the same as gradient exploding and gradient vanishing. 
				It indicates the difference in the feature size under each training strategy.<br>
				(2). Whether the optimization function or  norm is used, the exploding and vanishing problem always occurs and often leads to a decrease in classification accuracy. 
				Compare to the optimization function, the  norm can reflect the training process more intuitively and effectively.<br><br>

				<br><br>
				Reference:<br>
				[1] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross B. Girshick: Masked Autoencoders Are Scalable Vision Learners. CoRR abs/2111.06377 (2021)<br>
				[2] Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, Kaiming He: Masked Autoencoders As Spatiotemporal Learners. CoRR abs/2205.09113 (2022)<br>
				[3] Zhan Tong, Yibing Song, Jue Wang, Limin Wang: VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training. CoRR abs/2203.12602 (2022)<br>
				 </table>
		 
		 <br><br>
		 <hr>

		 <table align=center width=720px>
			<center><h1>Paper</h1></center>
			   <tr>
				 <td align=center><a href=""><img class="layered-paper-big" style="height:160px" src="./resources/images/Figure1.png"/></a></td>
				 <td><span style="font-size:14pt">Chen, Liu, Liu, Zhang, Zhang, Han, Mei.<br>
				 MAPLE: Masked Pseudo-Labeling autoEncoder for Semi-supervised Point Cloud Action Recognition<br>
				 In ACM MM, 2022 (Poster).<br>
				 (<a href="https://openaccess.thecvf.com/">arXiv</a>)</a>
				 <span style="font-size:4pt"><a href=""><br></a>
				 </span>
				 </td>
			 </tr>
		   </table>
		 
		 <br><br>
		 <hr>

		  <table align=center width=720px>
			<center><h1>Cite</h1></center>
		  <div class="disclaimerbox">
			<!-- <center><h2>How to interpret the results</h2></center> -->

		   <span>
				<!-- <center><span style="font-size:28px"><b>Cite</b></span></center> -->
				<pre style = "font-family:Courier; font-size:6px; text-align:left;">
				@inproceedings{chen2022MAPLE,
				title={MAPLE: Masked Pseudo-Labeling autoEncoder for Semi-supervised Point Cloud Action Recognition},
				author=Chen, Liu and Liu, Zhang and Han, Mei.},
				booktitle={ACM Multimedia (ACM MM)},
				year={2022}
				}
				</pre>
		    </span>
		  </div>
  		  </table>

			<br><br>
			<hr>
  
		  	
  		  <table align=center width=720px>
  			  <tr>
  	              <td width=400px>
  					<left>
	  		  <center><h1>Acknowledgements</h1></center>
				This work was supported by the National Key R&D Program of China under Grant No.2020AAA0103800.<br>
				This work was done when Xiaodong Chen was an intern at JD AI Research.<br>
</href><a>
			</left>
		</td>
			 </tr>
		</table>

		<br><br>
		<hr>

		<table align=center width=720px>
			<tr>
				<td width=400px>
				  <left>
			<center><h1>Contact</h1></center>
			For further questions and suggestions, please contact Xiaodong Chen (<a href='mailto:cxd1230@mail.ustc.edu.cn'>cxd1230@mail.ustc.edu.cn</a>).
			
			
		</left>
	</td>
		 </tr>
	</table>

		<br><br>

<script>
	
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75863369-1', 'auto');
  ga('send', 'pageview');

</script>
              
</body>
</html>
 
