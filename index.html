<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:16px;
		margin-left: auto;
		margin-right: auto;
		width: 800px;
	}
	
	h1 {
		font-weight:300;
	}
		
	h2 {
		font-weight:300;
		font-size: 22px;
		text-align: left;
	}

	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}

	pre {
    text-align: left;
    white-space: pre;
	background-color: ghostwhite;
	border: 1px solid #CCCCCC;
	padding: 10px 20px;
	margin: 10px;
    tab-size:         4; /* Chrome 21+, Safari 6.1+, Opera 15+ */
    -moz-tab-size:    4; /* Firefox 4+ */
    -o-tab-size:      4; /* Opera 11.5 & 12.1 only */
  	}

</style>

<html>
  <head>
		<title>MAPLE: Masked Pseudo-Labeling autoEncoder for Semi-supervised Point Cloud Action Recognition</title>
		<meta property="og:image" content=""/>
		<meta property="og:title" content="MAPLE: Masked Pseudo-Labeling autoEncoder for Semi-supervised Point Cloud Action Recognition" />
  </head>

  <body>
    <br>
          <center>
          	<span style="font-size:42px">MAPLE: Masked Pseudo-Labeling autoEncoder for Semi-supervised Point Cloud Action Recognition</span>
	  		  <table align=center width=900px>
	  			  <tr>
	  	              <td align=center width=170px>
	  					<center>
	  						<span style="font-size:20px"><a href="https://github.com/">Xiaodong Chen<sup>1,2</sup></a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=140px>
	  					<center>
	  						<span style="font-size:20px"><a href="https://github.com/">Xinchen Liu<sup>2</sup></a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=100px>
	  					<center>
	  						<span style="font-size:20px"><a href="https://github.com/">Wu Liu<sup>2</sup></a></span>
		  		  		</center>
		  		  	  </td>
					<td align=center width=170px>
						<center>
							<span style="font-size:20px"><a href="https://github.com/">Yongdong Zhang<sup>1</sup></a></span>
						</center>
					</td>

					<td align=center width=100px>
						<center>
							<span style="font-size:20px"><a href="https://github.com/">Tao Mei<sup>2</sup></a></span>
						</center>
					</td>
				</table>
          		<!-- <span style="font-size:30px">ACM MM 2022.</span> -->

			  <table align=center width=900px>
				  <tr>
					  <td align=center width=400px>
						<center>
							<span style="font-size:18px"><a href="https://github.com/"><sup>1</sup>University Of Science And Technology Of China</a></span>
						</center>
					  </td>
					  <td align=center width=250px>
						<center>
							<span style="font-size:18px"><a href="https://github.com/"><sup>2</sup>AI Research of JD.com</a></span>
						</center>
					  </td>

			  </table>
			  ACM Multimedia 2022 (<a href="https://2022.acmmm.org/" target="_blank">ACM MM</a>) 2022, <font color="#e86e14">Poster Presentation</font>
          </center>

   		  <br><br>
		  <hr>

  		  <br>
  		  <table align=center width=720px>
  			  <tr>
  	              <td width=400px>
  					<center>
  	                	<a href="./resources/images/Figure1.png" ><img class="rounded" src = "./resources/images/Figure1.png" width="95%" ></img></href></a><br>
					</center>
  	              </td>
                </tr>
  	              <td width=400px>
  					<center>
  	                	<span style="font-size:14px"><i>Figure 1</i>
					</center>
  	              </td>

  		  </table>
      	  <br><br>

		  <table align=center width=720px>
			<!-- <center><h1>Download</h1></center> -->
			<tr>
				<td width=300px>
					<center>
						<a href="#download"><img class="rounded" onmouseover="this.src='./resources/images/data_icon.png';" onmouseout="this.src='./resources/images/data_icon.png';" src = "./resources/images/data_icon.png" height = "120px"></a><br>
						<span style="font-size:16px">Download</span><br>
					</center>
				</td>
				<td width=300px>
					<center>
						<a href="#video"><img class="rounded" onmouseover="this.src='./resources/images/video_icon.png';" onmouseout="this.src='./resources/images/video_icon.png';" src = "./resources/images/video_icon.png" height = "120px"></a><br>
						<span style="font-size:16px">Videos at ACM MM'2022</span><br>
					</center>
				</td>
				<td width=300px>
					<center>
						<a href="#analysis"><img class="rounded" onmouseover="this.src='./resources/images/magnify_glass.png';" onmouseout="this.src='./resources/images/magnify_glass.png';" src = "./resources/images/magnify_glass.png" height = "120px"></a><br>
						<span style="font-size:16px">Analysis</span><br>
					</center>
				</td>
				<td width=300px>
					<center>
					  <a href="https://github.com/SheldongChen/MAPLE"><img class="rounded" onmouseover="this.src='./resources/images/github_icon.png';" onmouseout="this.src='./resources/images/github_icon.png';" src = "./resources/images/github_icon.png" height = "120px"></a><br>
					  <span style="font-size:16px">GitHub Repo</span><br>						
					</center>
				</td>
			</tr>
		  </table>

		  <br><br>

		  <hr>

  		  <table align=center width=720px>
				<center><h1>Abstract</h1></center>
		  </table>
		  <center>
			<span>
				Recognizing human actions from point cloud videos has attracted tremendous attention from both academia and industry due to its wide applications like automatic driving, robotics, and human-computer interaction.
				However, current methods for point cloud action recognition usually require a huge amount of data with manual annotations and a complex backbone network with high computation cost, which makes it impractical for real-world applications.
				Therefore, this paper considers the task of semi-supervised point cloud action recognition with an efficient model.
				To this end, we propose a Masked Pseudo-Labeling autoEncoder (<b>MAPLE</b>) framework to learn effective representations with much fewer annotations for point cloud action recognition.
				In particular, we design a novel and efficient <b>De</b>coupled <b>s</b>patial-<b>t</b>emporal Trans<b>Former</b> (<b>DestFormer</b>) as the backbone of MAPLE.
				In DestFormer, the spatial and temporal dimensions of the 4D point cloud videos are decoupled to achieve an efficient self-attention for learning both long-term and short-term features.
				Moreover, to learn discriminative features from fewer annotations, we design a masked pseudo-labeling autoencoder structure to guide the DestFormer to reconstruct features of masked frames from the available frames.
				More importantly, for unlabeled data, we exploit the pseudo-labels from the classification head as the supervision signal for the reconstruction of features from the masked frames.
				Finally, comprehensive experiments demonstrate that MAPLE achieves superior results on three public benchmarks and outperforms the state-of-the-art method by 8.08% accuracy on the MSR-Action3D dataset.
		  	</span>
		  </center>
  		  <br><br>
		  <hr>

		

		  <table align=center width=720px>
			<center><h1>5-Minute presentation video</h1></center>
			<tr>
				<table align=center width=720px>
					<tr>
						<td align=center width=720px>
							<iframe width="600" height="320" src="https://www.youtube.com/watch?v=" frameborder="0" allowfullscreen></iframe> 
						</td>
					  </tr>
					<tr>
					 </table>
			  </tr>
		  </table>
		   <br><br>
		  <hr>


		  <table align=center width=720px>
			<center><h1>Architecture</h1></center>
			<tr>
				<td width=400px>
				  <center>
					  <a><img class="rounded" src = "./resources/images/Figure2.png" width="800px"></img></a><br>
				</center>
				</td>
			</tr>
				<td width=400px>
				  <center>
					  <span style="font-size:14px"><i>
						The overall architecture of the attribute-guided metric distillation framework for person ReID. (a) The target ReID model that generates the pairwise distance for an image pair. (b) The interpret network that learns to decompose the pairwise distance into components of attributes and generates attention-guided attention maps for individual attributes. (Best viewed in color.)
						</i>
				</center>
				</td>

		  </table>
	      <br><br>
		  <hr>

		  <table align=center width=720px>
			<center><h1>Statistics of Attributes</h1></center>
			<tr>
				<center>
				<span>
					The statistics of attributes in Market-1501 and DukeMTMC-ReID are shown in Figures, which shows that the attributes are very imbalanced</span>
				<br>
				</center>
				<td width=360px>
					<center>
						<span style="font-size:22px"><a>Market-1501</a></span><br>
						<a><img class="rounded" src = "./resources/images/Figure3.PNG" width="360px"></img></a><br>
					</center>
				</td>
				<td width=360px>
					<center>
						<span style="font-size:22px"><a>DukeMTMC-ReID</a></span><br>
						<a><img class="rounded" src = "./resources/images/Figure5.PNG" width="360px"></img></a><br>
					</center>
				</td>
			</tr>
		  </table>

		  <br><br>
		  <hr>

		  <table id='analysis' align=center width=720px>
			<center><h1>Evaluation and Visualization</h1></center>
			<table>
			<center><h2> (1) Evaluation of interpreters for different backbone models on Market-1501 and DukeMTMC-ReID.  </h2></center>
			<tr>
				  <td width=400px>
				  <center>
					  <a><img class="rounded" src = "./resources/images/Table1.PNG" width="800px"></img></a><br>
				</center>
				</td>
			</tr>
			<tr>
				<td align=center width=720px>
				  <span style="font-size:14px"><i>
					Each target model and the corresponding interpreter are grouped for comparison.</i>
				</span>
				</td>
			</tr>
			</table>

			<br>

			<table>
			<center><h2> (2) Pairwise examples and explanations for SBS (ResNet-50) on two datasets. </h2></center>
			<tr>
				  <td width=400px>
				  <center>
					  <a><img class="rounded" src = "./resources/images/Figure4.PNG" width="800px"></img></a><br>
				</center>
				</td>
			</tr>
			<tr>
				<td align=center width=720px>
				  <span style="font-size:14px"><i>
					For each pair of images, the upper part visualizes the
					AAMs of the top-3 attributes, which shows that the AAMs are attended to the discriminative attributes. The lower part shows the overall
					distance and contributions of the top-3 attributes. These figures show the most contributed attributes discovered by the interpreter.</i>
				</span>
				</td>
			</tr>
			</table>

			<br>

			<table>
				<center><h2> (3) Under the cross-domain setting? </h2></center>
				<tr>
					  <td width=400px>
					  <center>
						  <a><img class="rounded" src = "./resources/images/Table2.PNG" width="800px"></img></a><br>
					</center>
					</td>
				</tr>
				<tr>
					<td align=center width=720px>
					  <span style="font-size:14px"><i>
						Evaluation of the interpreters for SBS (ResNet-50) under the cross-domain setting. 
						M to D means the SBS models and interpreters are trained on Market-1501 and tested on DukeMTMC-ReID, and D to M means the reverse setting. 
						The results demonstrate that the information loss of interpreters is very minor under the cross-domain setting.</i>
					</span>
					</td>
				</tr>
				</table>

			
		
					
		</table>
	      <br><br>
		  <hr>


		  <table id="download" align=center width=720px>
			<center><h1>Download</h1></center>
			<tr>
		
				<td width=300px>
					<center>
						<span style="font-size:24px">Log of train and test</span><br><br>
						<img class="rounded" onmouseover="this.src='./resources/images/dataset_icon.jpg';" onmouseout="this.src='./resources/images/dataset_icon.jpg';" src = "./resources/images/dataset_icon.jpg" height = "150px"><br><br>
						<span style="font-size:16px"><a href='resources/dataset/'>TBD</a></span><br>
						<span style="font-size:16px"><a href='resources/dataset/'>TBD</a></span><br>
					<span style="font-size:16px"></span>
					</center>
				</td>
				<table>
				<center><h2> Updates </h2></center>
				[30/06/2022] We include new subsections to track updates and address FAQs.<br>
				<center><h2> FAQs </h2></center>
				Q0: TBD:<br>
				A0: TBD.<br>
				 </table>
		 
		 <br><br>
		 <hr>

		 <table align=center width=720px>
			<center><h1>Paper</h1></center>
			   <tr>
				 <td align=center><a href=""><img class="layered-paper-big" style="height:160px" src="./resources/images/Figure6.png"/></a></td>
				 <td><span style="font-size:14pt">Chen, Liu, Liu, Zhang, Zhang, Mei.<br>
				 MAPLE: Masked Pseudo-Labeling autoEncoder for Semi-supervised Point Cloud Action Recognition<br>
				 In ACM MM, 2022 (Poster).<br>
				 (<a href="https://openaccess.thecvf.com/content/ACMMM2022/papers/Chen_Explainable_Person_Re-Identification_With_Attribute-Guided_Metric_Distillation_ACMMM_2022_paper.pdf">arXiv</a>)</a>
				 <span style="font-size:4pt"><a href=""><br></a>
				 </span>
				 </td>
				 <td align=center><a href=""><img class="layered-paper-big" style="height:160px" src="./resources/images/Figure7.png"/></a></td>
				 <td><span style="font-size:14pt">
				 (<a href="https://openaccess.thecvf.com/content/ACMMM2022/supplemental/Chen_Explainable_Person_Re-Identification_ACMMM_2022_supplemental.pdf">Additional details/<br>supplementary materials</a>)</a>
				 <span style="font-size:4pt"><a href=""><br></a>
				 </span>
				 </td>
			 </tr>
		   </table>
		 
		 <br><br>
		 <hr>

		  <table align=center width=720px>
			<center><h1>Cite</h1></center>
		  <div class="disclaimerbox">
			<!-- <center><h2>How to interpret the results</h2></center> -->

		   <span>
				<!-- <center><span style="font-size:28px"><b>Cite</b></span></center> -->
				<pre style = "font-family:Courier; font-size:14px">
				@inproceedings{chen2022MAPLE,
				title={MAPLE: Masked Pseudo-Labeling autoEncoder for Semi-supervised Point Cloud Action Recognition},
				author=Chen, Liu and Liu, Zhang and Zhang, Mei.},
				booktitle={ACM Multimedia (ACM MM)},
				year={2022}
				}
				</pre>
		  </div>
  		  </table>

			<br><br>
			<hr>
  
		  	
  		  <table align=center width=720px>
  			  <tr>
  	              <td width=400px>
  					<left>
	  		  <center><h1>Acknowledgements</h1></center>
				This work was supported by the National Key R&D Program of China under Grant No.2020AAA0103800.<br>
				This work was done when Xiaodong Chen was an intern at JD AI Research.<br>
</href><a>
			</left>
		</td>
			 </tr>
		</table>

		<br><br>
		<hr>

		<table align=center width=720px>
			<tr>
				<td width=400px>
				  <left>
			<center><h1>Contact</h1></center>
			For further questions and suggestions, please contact Xiaodong Chen (<a href='mailto:cxd1230@mail.ustc.edu.cn'>cxd1230@mail.ustc.edu.cn</a>).
			
			
		</left>
	</td>
		 </tr>
	</table>

		<br><br>

<script>
	
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75863369-1', 'auto');
  ga('send', 'pageview');

</script>
              
</body>
</html>
 
